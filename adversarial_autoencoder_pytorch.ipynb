{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yastiaisyah/DataSynthesis/blob/main/adversarial_autoencoder_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rbeetERQ76X",
        "outputId": "1efa5ce4-9f84-4deb-ab2a-a1253fe7a6f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 74516856.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 76571235.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 31893395.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 989946.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter-0; D_loss: 1.397; G_loss: 0.6724; recon_loss: 0.6947\n",
            "Iter-1000; D_loss: 1.142; G_loss: 0.9264; recon_loss: 0.2744\n",
            "Iter-2000; D_loss: 1.317; G_loss: 0.9261; recon_loss: 0.2518\n",
            "Iter-3000; D_loss: 1.863; G_loss: 0.8282; recon_loss: 0.2677\n",
            "Iter-4000; D_loss: 1.357; G_loss: 0.8837; recon_loss: 0.2719\n",
            "Iter-5000; D_loss: 0.8553; G_loss: 0.9618; recon_loss: 0.2588\n",
            "Iter-6000; D_loss: 1.081; G_loss: 0.7807; recon_loss: 0.2753\n",
            "Iter-7000; D_loss: 1.182; G_loss: 1.382; recon_loss: 0.263\n",
            "Iter-8000; D_loss: 1.319; G_loss: 0.8952; recon_loss: 0.2699\n",
            "Iter-9000; D_loss: 4.415; G_loss: 0.03042; recon_loss: 0.2745\n",
            "Iter-10000; D_loss: 0.7709; G_loss: 1.238; recon_loss: 0.2598\n",
            "Iter-11000; D_loss: 0.5094; G_loss: 1.451; recon_loss: 0.277\n",
            "Iter-12000; D_loss: 1.437; G_loss: 1.081; recon_loss: 0.253\n",
            "Iter-13000; D_loss: 0.7438; G_loss: 1.52; recon_loss: 0.2537\n",
            "Iter-14000; D_loss: 1.355; G_loss: 0.7209; recon_loss: 0.2447\n",
            "Iter-15000; D_loss: 1.092; G_loss: 0.867; recon_loss: 0.2568\n",
            "Iter-16000; D_loss: 1.648; G_loss: 0.6585; recon_loss: 0.2525\n",
            "Iter-17000; D_loss: 1.215; G_loss: 0.7813; recon_loss: 0.2486\n",
            "Iter-18000; D_loss: 1.51; G_loss: 0.4819; recon_loss: 0.261\n",
            "Iter-19000; D_loss: 1.565; G_loss: 0.6244; recon_loss: 0.2397\n",
            "Iter-20000; D_loss: 1.002; G_loss: 1.215; recon_loss: 0.2679\n",
            "Iter-21000; D_loss: 1.727; G_loss: 0.6574; recon_loss: 0.2702\n",
            "Iter-22000; D_loss: 1.172; G_loss: 0.8611; recon_loss: 0.2813\n",
            "Iter-23000; D_loss: 1.539; G_loss: 0.6062; recon_loss: 0.2772\n",
            "Iter-24000; D_loss: 1.474; G_loss: 0.8054; recon_loss: 0.25\n",
            "Iter-25000; D_loss: 1.315; G_loss: 0.7169; recon_loss: 0.2554\n",
            "Iter-26000; D_loss: 1.963; G_loss: 0.7492; recon_loss: 0.2449\n",
            "Iter-27000; D_loss: 2.257; G_loss: 0.4646; recon_loss: 0.2709\n",
            "Iter-28000; D_loss: 1.134; G_loss: 0.996; recon_loss: 0.2548\n",
            "Iter-29000; D_loss: 3.233; G_loss: 0.08228; recon_loss: 0.2907\n",
            "Iter-30000; D_loss: 2.083; G_loss: 1.398; recon_loss: 0.2701\n",
            "Iter-31000; D_loss: 0.9787; G_loss: 1.456; recon_loss: 0.2514\n",
            "Iter-32000; D_loss: 1.229; G_loss: 0.7156; recon_loss: 0.2637\n",
            "Iter-33000; D_loss: 1.891; G_loss: 0.4026; recon_loss: 0.2697\n",
            "Iter-34000; D_loss: 1.513; G_loss: 1.464; recon_loss: 0.2635\n",
            "Iter-35000; D_loss: 1.261; G_loss: 0.8617; recon_loss: 0.2506\n",
            "Iter-36000; D_loss: 0.8856; G_loss: 1.027; recon_loss: 0.2541\n",
            "Iter-37000; D_loss: 1.052; G_loss: 1.345; recon_loss: 0.2499\n",
            "Iter-38000; D_loss: 1.497; G_loss: 1.01; recon_loss: 0.2495\n",
            "Iter-39000; D_loss: 1.043; G_loss: 0.9895; recon_loss: 0.2673\n",
            "Iter-40000; D_loss: 1.179; G_loss: 1.04; recon_loss: 0.2846\n",
            "Iter-41000; D_loss: 1.976; G_loss: 1.09; recon_loss: 0.2587\n",
            "Iter-42000; D_loss: 1.51; G_loss: 0.7365; recon_loss: 0.2394\n",
            "Iter-43000; D_loss: 2.39; G_loss: 0.2166; recon_loss: 0.2898\n",
            "Iter-44000; D_loss: 1.511; G_loss: 0.6016; recon_loss: 0.2748\n",
            "Iter-45000; D_loss: 2.518; G_loss: 0.1805; recon_loss: 0.2442\n",
            "Iter-46000; D_loss: 2.98; G_loss: 0.1318; recon_loss: 0.3367\n",
            "Iter-47000; D_loss: 1.394; G_loss: 0.7955; recon_loss: 0.2643\n",
            "Iter-48000; D_loss: 1.511; G_loss: 0.5106; recon_loss: 0.2609\n",
            "Iter-49000; D_loss: 1.503; G_loss: 0.6479; recon_loss: 0.261\n",
            "Iter-50000; D_loss: 1.298; G_loss: 1.253; recon_loss: 0.2881\n",
            "Iter-51000; D_loss: 1.244; G_loss: 0.8454; recon_loss: 0.2782\n",
            "Iter-52000; D_loss: 1.239; G_loss: 0.6025; recon_loss: 0.2536\n",
            "Iter-53000; D_loss: 2.656; G_loss: 0.699; recon_loss: 0.2713\n",
            "Iter-54000; D_loss: 0.911; G_loss: 1.069; recon_loss: 0.2655\n",
            "Iter-55000; D_loss: 1.317; G_loss: 0.8377; recon_loss: 0.2656\n",
            "Iter-56000; D_loss: 1.143; G_loss: 0.7144; recon_loss: 0.2426\n",
            "Iter-57000; D_loss: 1.289; G_loss: 0.6227; recon_loss: 0.2608\n",
            "Iter-58000; D_loss: 1.031; G_loss: 0.7241; recon_loss: 0.2575\n",
            "Iter-59000; D_loss: 1.475; G_loss: 0.6462; recon_loss: 0.281\n",
            "Iter-60000; D_loss: 2.135; G_loss: 0.2659; recon_loss: 0.269\n",
            "Iter-61000; D_loss: 1.582; G_loss: 0.5481; recon_loss: 0.2586\n",
            "Iter-62000; D_loss: 1.645; G_loss: 0.6571; recon_loss: 0.2725\n",
            "Iter-63000; D_loss: 1.775; G_loss: 0.6078; recon_loss: 0.2601\n",
            "Iter-64000; D_loss: 1.545; G_loss: 1.551; recon_loss: 0.2521\n",
            "Iter-65000; D_loss: 1.183; G_loss: 0.8293; recon_loss: 0.263\n",
            "Iter-66000; D_loss: 0.9762; G_loss: 1.53; recon_loss: 0.2571\n",
            "Iter-67000; D_loss: 1.466; G_loss: 1.149; recon_loss: 0.2482\n",
            "Iter-68000; D_loss: 2.227; G_loss: 0.3224; recon_loss: 0.2453\n",
            "Iter-69000; D_loss: 2.64; G_loss: 0.7405; recon_loss: 0.259\n",
            "Iter-70000; D_loss: 1.217; G_loss: 0.6926; recon_loss: 0.2523\n",
            "Iter-71000; D_loss: 0.9206; G_loss: 1.036; recon_loss: 0.2617\n",
            "Iter-72000; D_loss: 1.133; G_loss: 0.8138; recon_loss: 0.2932\n",
            "Iter-73000; D_loss: 1.069; G_loss: 1.038; recon_loss: 0.2824\n",
            "Iter-74000; D_loss: 0.9888; G_loss: 1.192; recon_loss: 0.2768\n",
            "Iter-75000; D_loss: 1.554; G_loss: 1.245; recon_loss: 0.2591\n",
            "Iter-76000; D_loss: 2.223; G_loss: 1.002; recon_loss: 0.2642\n",
            "Iter-77000; D_loss: 1.749; G_loss: 0.4707; recon_loss: 0.2571\n",
            "Iter-78000; D_loss: 1.171; G_loss: 1.217; recon_loss: 0.2401\n",
            "Iter-79000; D_loss: 1.905; G_loss: 0.3935; recon_loss: 0.2485\n",
            "Iter-80000; D_loss: 1.2; G_loss: 0.8883; recon_loss: 0.2518\n",
            "Iter-81000; D_loss: 1.643; G_loss: 0.6596; recon_loss: 0.2781\n",
            "Iter-82000; D_loss: 0.9587; G_loss: 1.12; recon_loss: 0.2775\n",
            "Iter-83000; D_loss: 1.171; G_loss: 1.242; recon_loss: 0.2622\n",
            "Iter-84000; D_loss: 1.491; G_loss: 0.6592; recon_loss: 0.2771\n",
            "Iter-85000; D_loss: 1.586; G_loss: 0.3879; recon_loss: 0.2755\n",
            "Iter-86000; D_loss: 1.259; G_loss: 0.8883; recon_loss: 0.2595\n",
            "Iter-87000; D_loss: 1.602; G_loss: 0.9293; recon_loss: 0.2497\n",
            "Iter-88000; D_loss: 1.284; G_loss: 1.267; recon_loss: 0.2842\n",
            "Iter-89000; D_loss: 1.526; G_loss: 0.5635; recon_loss: 0.2646\n",
            "Iter-90000; D_loss: 2.955; G_loss: 0.1317; recon_loss: 0.2409\n",
            "Iter-91000; D_loss: 1.128; G_loss: 1.044; recon_loss: 0.2757\n",
            "Iter-92000; D_loss: 1.577; G_loss: 0.5974; recon_loss: 0.271\n",
            "Iter-93000; D_loss: 1.496; G_loss: 0.9005; recon_loss: 0.2525\n",
            "Iter-94000; D_loss: 1.347; G_loss: 1.219; recon_loss: 0.2553\n",
            "Iter-95000; D_loss: 1.679; G_loss: 0.8947; recon_loss: 0.2619\n",
            "Iter-96000; D_loss: 1.256; G_loss: 1.486; recon_loss: 0.2455\n",
            "Iter-97000; D_loss: 1.302; G_loss: 0.9303; recon_loss: 0.2624\n",
            "Iter-98000; D_loss: 0.6132; G_loss: 2.074; recon_loss: 0.2741\n",
            "Iter-99000; D_loss: 0.9233; G_loss: 2.238; recon_loss: 0.2656\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import os\n",
        "from torch.autograd import Variable\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "\n",
        "# Define data transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load MNIST dataset\n",
        "mnist = MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "\n",
        "# Normalize the data to [0, 1] range\n",
        "mnist.data = mnist.data.float() / 255.0\n",
        "\n",
        "# Parameters\n",
        "mb_size = 32\n",
        "z_dim = 5\n",
        "X_dim = mnist.data.size(1) * mnist.data.size(2)  # Flattened image dimensions\n",
        "h_dim = 128\n",
        "lr = 1e-3\n",
        "\n",
        "# Encoder\n",
        "Q = nn.Sequential(\n",
        "    nn.Linear(X_dim, h_dim),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(h_dim, z_dim)\n",
        ")\n",
        "\n",
        "# Decoder\n",
        "P = nn.Sequential(\n",
        "    nn.Linear(z_dim, h_dim),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(h_dim, X_dim),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "# Discriminator\n",
        "D = nn.Sequential(\n",
        "    nn.Linear(z_dim, h_dim),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(h_dim, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "def reset_grad():\n",
        "    Q.zero_grad()\n",
        "    P.zero_grad()\n",
        "    D.zero_grad()\n",
        "\n",
        "def sample_X(size):\n",
        "    indices = np.random.randint(0, len(mnist), size)\n",
        "    X = mnist.data[indices].view(size, -1).float()\n",
        "    return Variable(X)\n",
        "\n",
        "Q_solver = optim.Adam(Q.parameters(), lr=lr)\n",
        "P_solver = optim.Adam(P.parameters(), lr=lr)\n",
        "D_solver = optim.Adam(D.parameters(), lr=lr)\n",
        "cnt=0\n",
        "\"\"\"1000000\"\"\"\n",
        "for it in range(100000):\n",
        "    X = sample_X(mb_size)\n",
        "\n",
        "    \"\"\" Reconstruction phase \"\"\"\n",
        "    z_sample = Q(X)\n",
        "    X_sample = P(z_sample)\n",
        "\n",
        "    # Clip values to be within [0, 1]\n",
        "    X_sample = X_sample.clamp(0, 1)\n",
        "\n",
        "    # Use BCELoss for binary cross entropy\n",
        "    recon_loss = nn.BCELoss()(X_sample, X)\n",
        "\n",
        "    recon_loss.backward()\n",
        "    P_solver.step()\n",
        "    Q_solver.step()\n",
        "    reset_grad()\n",
        "\n",
        "    \"\"\" Regularization phase \"\"\"\n",
        "    # Discriminator\n",
        "    z_real = Variable(torch.randn(mb_size, z_dim))\n",
        "    z_fake = Q(X)\n",
        "\n",
        "    D_real = D(z_real)\n",
        "    D_fake = D(z_fake)\n",
        "\n",
        "    D_loss = -torch.mean(torch.log(D_real) + torch.log(1 - D_fake))\n",
        "\n",
        "    D_loss.backward()\n",
        "    D_solver.step()\n",
        "    reset_grad()\n",
        "\n",
        "    # Generator\n",
        "    z_fake = Q(X)\n",
        "    D_fake = D(z_fake)\n",
        "\n",
        "    G_loss = -torch.mean(torch.log(D_fake))\n",
        "\n",
        "    G_loss.backward()\n",
        "    Q_solver.step()\n",
        "    reset_grad()\n",
        "\n",
        "    # Print and plot every now and then\n",
        "    if it % 1000 == 0:\n",
        "        print('Iter-{}; D_loss: {:.4}; G_loss: {:.4}; recon_loss: {:.4}'\n",
        "              .format(it, D_loss.item(), G_loss.item(), recon_loss.item()))\n",
        "\n",
        "        samples = P(z_real).data.numpy()[:16]\n",
        "\n",
        "        fig = plt.figure(figsize=(4, 4))\n",
        "        gs = gridspec.GridSpec(4, 4)\n",
        "        gs.update(wspace=0.05, hspace=0.05)\n",
        "\n",
        "        for i, sample in enumerate(samples):\n",
        "            ax = plt.subplot(gs[i])\n",
        "            plt.axis('off')\n",
        "            ax.set_xticklabels([])\n",
        "            ax.set_yticklabels([])\n",
        "            ax.set_aspect('equal')\n",
        "            plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
        "\n",
        "        if not os.path.exists('out/'):\n",
        "            os.makedirs('out/')\n",
        "\n",
        "        plt.savefig('out/{}.png'\n",
        "                    .format(str(cnt).zfill(3)), bbox_inches='tight')\n",
        "        cnt += 1\n",
        "        plt.close(fig)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}