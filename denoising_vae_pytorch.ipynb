{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yastiaisyah/DataSynthesis/blob/main/denoising_vae_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5IZHbcutj0F",
        "outputId": "5929db80-2f9c-4c03-9fac-b5304579dea4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 73145678.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 7538002.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 32333869.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 9963665.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter-0; Loss: 769.7\n",
            "Iter-1000; Loss: 153.5\n",
            "Iter-2000; Loss: 135.9\n",
            "Iter-3000; Loss: 123.4\n",
            "Iter-4000; Loss: 134.7\n",
            "Iter-5000; Loss: 135.8\n",
            "Iter-6000; Loss: 141.2\n",
            "Iter-7000; Loss: 133.8\n",
            "Iter-8000; Loss: 150.0\n",
            "Iter-9000; Loss: 133.0\n",
            "Iter-10000; Loss: 124.1\n",
            "Iter-11000; Loss: 132.4\n",
            "Iter-12000; Loss: 135.5\n",
            "Iter-13000; Loss: 136.3\n",
            "Iter-14000; Loss: 133.4\n",
            "Iter-15000; Loss: 134.0\n",
            "Iter-16000; Loss: 139.9\n",
            "Iter-17000; Loss: 134.5\n",
            "Iter-18000; Loss: 136.4\n",
            "Iter-19000; Loss: 116.4\n",
            "Iter-20000; Loss: 123.2\n",
            "Iter-21000; Loss: 129.3\n",
            "Iter-22000; Loss: 137.4\n",
            "Iter-23000; Loss: 126.1\n",
            "Iter-24000; Loss: 127.1\n",
            "Iter-25000; Loss: 126.6\n",
            "Iter-26000; Loss: 131.3\n",
            "Iter-27000; Loss: 122.3\n",
            "Iter-28000; Loss: 126.7\n",
            "Iter-29000; Loss: 128.5\n",
            "Iter-30000; Loss: 133.5\n",
            "Iter-31000; Loss: 137.9\n",
            "Iter-32000; Loss: 134.8\n",
            "Iter-33000; Loss: 140.6\n",
            "Iter-34000; Loss: 136.3\n",
            "Iter-35000; Loss: 124.8\n",
            "Iter-36000; Loss: 133.2\n",
            "Iter-37000; Loss: 134.5\n",
            "Iter-38000; Loss: 128.9\n",
            "Iter-39000; Loss: 126.6\n",
            "Iter-40000; Loss: 133.7\n",
            "Iter-41000; Loss: 125.7\n",
            "Iter-42000; Loss: 136.7\n",
            "Iter-43000; Loss: 114.0\n",
            "Iter-44000; Loss: 131.6\n",
            "Iter-45000; Loss: 124.0\n",
            "Iter-46000; Loss: 118.1\n",
            "Iter-47000; Loss: 128.1\n",
            "Iter-48000; Loss: 115.4\n",
            "Iter-49000; Loss: 124.5\n",
            "Iter-50000; Loss: 139.4\n",
            "Iter-51000; Loss: 123.9\n",
            "Iter-52000; Loss: 126.0\n",
            "Iter-53000; Loss: 129.1\n",
            "Iter-54000; Loss: 130.9\n",
            "Iter-55000; Loss: 122.0\n",
            "Iter-56000; Loss: 121.6\n",
            "Iter-57000; Loss: 124.6\n",
            "Iter-58000; Loss: 127.9\n",
            "Iter-59000; Loss: 138.2\n",
            "Iter-60000; Loss: 111.5\n",
            "Iter-61000; Loss: 130.5\n",
            "Iter-62000; Loss: 125.8\n",
            "Iter-63000; Loss: 141.7\n",
            "Iter-64000; Loss: 112.9\n",
            "Iter-65000; Loss: 129.4\n",
            "Iter-66000; Loss: 127.9\n",
            "Iter-67000; Loss: 124.0\n",
            "Iter-68000; Loss: 130.2\n",
            "Iter-69000; Loss: 128.5\n",
            "Iter-70000; Loss: 114.0\n",
            "Iter-71000; Loss: 129.2\n",
            "Iter-72000; Loss: 125.1\n",
            "Iter-73000; Loss: 117.4\n",
            "Iter-74000; Loss: 137.1\n",
            "Iter-75000; Loss: 136.9\n",
            "Iter-76000; Loss: 123.4\n",
            "Iter-77000; Loss: 124.7\n",
            "Iter-78000; Loss: 156.4\n",
            "Iter-79000; Loss: 133.8\n",
            "Iter-80000; Loss: 123.1\n",
            "Iter-81000; Loss: 124.6\n",
            "Iter-82000; Loss: 124.2\n",
            "Iter-83000; Loss: 138.1\n",
            "Iter-84000; Loss: 127.7\n",
            "Iter-85000; Loss: 123.8\n",
            "Iter-86000; Loss: 124.7\n",
            "Iter-87000; Loss: 130.9\n",
            "Iter-88000; Loss: 131.1\n",
            "Iter-89000; Loss: 138.1\n",
            "Iter-90000; Loss: 137.0\n",
            "Iter-91000; Loss: 123.1\n",
            "Iter-92000; Loss: 118.7\n",
            "Iter-93000; Loss: 130.3\n",
            "Iter-94000; Loss: 121.1\n",
            "Iter-95000; Loss: 132.5\n",
            "Iter-96000; Loss: 128.0\n",
            "Iter-97000; Loss: 125.5\n",
            "Iter-98000; Loss: 119.3\n",
            "Iter-99000; Loss: 127.1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "import torch.nn.init as init\n",
        "\n",
        "# Define data transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load MNIST dataset\n",
        "mnist = MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "\n",
        "# Normalize the data to [0, 1] range\n",
        "mnist.data = mnist.data.float() / 255.0\n",
        "\n",
        "# Parameters\n",
        "mb_size = 32\n",
        "z_dim = 5\n",
        "X_dim = mnist.data.size(1) * mnist.data.size(2)  # Flattened image dimensions\n",
        "h_dim = 128\n",
        "lr = 1e-3\n",
        "noise_factor = .25\n",
        "\n",
        "# Define Q, P, sample_z, and other functions here\n",
        "\n",
        "def xavier_init(size):\n",
        "    in_dim = size[0]\n",
        "    xavier_stddev = 1. / np.sqrt(in_dim / 2.)\n",
        "    return Variable(torch.randn(*size) * xavier_stddev, requires_grad=True)\n",
        "\n",
        "# Define Q(z|X)\n",
        "Whz_mu = xavier_init(size=[h_dim, z_dim])\n",
        "bhz_mu = Variable(torch.zeros(z_dim), requires_grad=True)\n",
        "\n",
        "Whz_var = xavier_init(size=[h_dim, z_dim])\n",
        "bhz_var = Variable(torch.zeros(z_dim), requires_grad=True)\n",
        "\n",
        "def sample_z(mu, log_var):\n",
        "    eps = Variable(torch.randn(mu.size()))\n",
        "    return mu + torch.exp(log_var / 2) * eps\n",
        "\n",
        "def Q(X):\n",
        "    h = nn.ReLU()(X @ Wxh + bxh.repeat(X.size(0), 1))\n",
        "    z_mu = h @ Whz_mu + bhz_mu.repeat(h.size(0), 1)\n",
        "    z_var = h @ Whz_var + bhz_var.repeat(h.size(0), 1)\n",
        "    return z_mu, z_var\n",
        "\n",
        "# Define P(X|z)\n",
        "Wzh = xavier_init(size=[z_dim, h_dim])\n",
        "bzh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
        "\n",
        "Whx = xavier_init(size=[h_dim, X_dim])\n",
        "bhx = Variable(torch.zeros(X_dim), requires_grad=True)\n",
        "\n",
        "Wxh = xavier_init(size=[X_dim, h_dim])\n",
        "bxh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
        "\n",
        "def P(z):\n",
        "    h = nn.ReLU()(z @ Wzh + bzh.repeat(z.size(0), 1))\n",
        "    X = nn.Sigmoid()(h @ Whx + bhx.repeat(h.size(0), 1))\n",
        "    return X\n",
        "\n",
        "\n",
        "# Training\n",
        "params = [Wxh, bxh, Whz_mu, bhz_mu, Whz_var, bhz_var,\n",
        "          Wzh, bzh, Whx, bhx]\n",
        "\n",
        "solver = optim.Adam(params, lr=lr)\n",
        "c = 0  # Counter for saving images\n",
        "\n",
        "\"\"\"1000000\"\"\"\n",
        "for it in range(100000):\n",
        "    indices = torch.LongTensor(np.random.choice(mnist.data.size(0), mb_size))\n",
        "    X = mnist.data[indices].view(mb_size, -1)  # Flatten the input image\n",
        "\n",
        "    X = Variable(X)\n",
        "\n",
        "    # Add noise\n",
        "    X_noise = X + noise_factor * Variable(torch.randn(X.size()))\n",
        "    X_noise.data.clamp_(0., 1.)\n",
        "\n",
        "    # Forward\n",
        "    z_mu, z_var = Q(X_noise)\n",
        "    z = sample_z(z_mu, z_var)\n",
        "    X_sample = P(z)\n",
        "\n",
        "    # Loss calculation\n",
        "    recon_loss = F.binary_cross_entropy(X_sample, X, size_average=False) / mb_size\n",
        "    kl_loss = torch.mean(0.5 * torch.sum(torch.exp(z_var) + z_mu**2 - 1. - z_var, 1))\n",
        "    loss = recon_loss + kl_loss\n",
        "\n",
        "    # Backward\n",
        "    solver.zero_grad()\n",
        "    loss.backward()\n",
        "    solver.step()\n",
        "\n",
        "    # Print and plot every now and then\n",
        "    if it % 1000 == 0:\n",
        "        print('Iter-{}; Loss: {:.4}'.format(it, loss.item()))\n",
        "\n",
        "        z = Variable(torch.randn(mb_size, z_dim))\n",
        "        samples = P(z).data.numpy()[:16]\n",
        "\n",
        "        fig = plt.figure(figsize=(4, 4))\n",
        "        gs = gridspec.GridSpec(4, 4)\n",
        "        gs.update(wspace=0.05, hspace=0.05)\n",
        "\n",
        "        for i, sample in enumerate(samples):\n",
        "            ax = plt.subplot(gs[i])\n",
        "            plt.axis('off')\n",
        "            ax.set_xticklabels([])\n",
        "            ax.set_yticklabels([])\n",
        "            ax.set_aspect('equal')\n",
        "            plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
        "\n",
        "        if not os.path.exists('out/'):\n",
        "            os.makedirs('out/')\n",
        "\n",
        "        plt.savefig('out/{}.png'.format(str(c).zfill(3)), bbox_inches='tight')\n",
        "        c += 1\n",
        "        plt.close(fig)"
      ]
    }
  ]
}